---
title: "STA286 Lecture 17"
author: "Neil Montgomery"
date: "Last edited: `r format(Sys.time(), '%Y-%m-%d %H:%M')`"
output: 
  beamer_presentation:
    keep_tex: TRUE
    incremental: TRUE
    df_print: tibble
    fig_caption: FALSE
classoption: aspectratio=169
header-includes:
- \renewcommand{\le}{\leqslant}
- \renewcommand{\ge}{\geqslant}
- \renewcommand\P[1]{P{\left(#1\right)}}
- \newcommand\F[1]{F_{\tiny{#1}}}
- \newcommand\f[1]{f_{\tiny{#1}}}
- \newcommand\p[1]{p_{\tiny{#1}}}
- \newcommand\M[1]{M_{\tiny{#1}}}
- \newcommand\V[1]{\text{Var}\!\left(#1\right)}
- \newcommand\E[1]{E\!\left(#1\right)}
- \newcommand\N[1]{N_{\tiny{#1}}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
options(tibble.width=70)
```

## waiting time to the $n^{th}$ event of a Poisson process

Let's say we have a Poisson process $N(t)$ with rate $\lambda$. The time of the ~~first~~ $n^{th}$ event is random. Call this time $X$.

What can we say about $X$? Can we completely describe its distribution? 

\pause Yes, because $F(t) = 1 - P(X > t)$, and $\{X > t\}$ *is exactly equivalent to*  $\{N(t) \le n-1\},$ so we can derive the cdf for $X$. 

\pause $$F(t) = P(X \le t) = \begin{cases}
0 &: t \le 0\\
1 - \sum_{i=0}^{n-1} \frac{[\lambda t]^{i}}{i!}e^{-\lambda t} &: t > 0\\
\end{cases}.$$


\pause So the density is (a long telescoping sum of work later...):
$$f(t) = F'(t) = \begin{cases}
\frac{\lambda^n}{(n-1)!} t^{n-1} e^{-\lambda t} &: t > 0\\
0 &: \text{otherwise}.
\end{cases}$$

## the gamma distributions

The density is a special class of a larger family of distributions. 

\pause Definition: the *gamma function* is defined as:
$$\Gamma(\alpha) = \int_0^\infty u^{\alpha-1}e^{-u}\,du, \qquad \alpha > 0.$$
Many interesting properties, including $\Gamma(n) = (n-1)!$ for integer $n\ge 1$.

\pause The following function is a valid density for $\alpha > 0$ and $\lambda > 0$:
$$f(x) = \begin{cases}
\frac{\lambda^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\lambda x}
&: x > 0\\
0 &: \text{ otherwise}.\end{cases}$$

\pause The parameters $\alpha$ and $\lambda$ are called the \textit{shape} and \text{rate} parameters. We say $X \sim \text{Gamma}(\alpha, \lambda)$

\pause $\alpha=1$ is the special case of Exp$(\lambda)$.

## pictures of some Gamma($\alpha$, 1) densities

```{r, echo=FALSE, message=FALSE, warning=FALSE}

library(dplyr)
library(ggplot2)

x <- seq(0,12, length.out = 1000)

g_d <- rbind(data_frame(alpha=0.5, x, 'f(x)' = dgamma(x, 0.5, 1)),
      data_frame(alpha = 1, x, 'f(x)' = dgamma(x, 1, 1)),
      data_frame(alpha = 2, x, 'f(x)' = dgamma(x, 2, 1)),
      data_frame(alpha = 5, x, 'f(x)' = dgamma(x, 5, 1)))

g_d$alpha <- factor(g_d$alpha)

ggplot(g_d, aes(x=x, y=`f(x)`, color = alpha)) + 
  geom_line() + ylim(0,1.2) + theme_bw()
```

## properties of gamma distributions

If $X \sim \text{Gamma}(\alpha, \lambda)$, its moment generating function can be found to be:
$$\M{X}(s) = \left(\frac{\lambda}{\lambda-s}\right)^\alpha$$

\pause (so the mean and variance are $\frac{\alpha}{\lambda}$ and $\frac{\alpha}{\lambda^2}$)

\pause Suppose $X_1,X_2,\ldots,X_n$ are i.i.d. Exp$(\lambda)$. What is the distribution of $X = X_1 + X_2 + \cdots + X_n$?

\pause Using the m.g.f. argument it is clear that $X \sim \text{Gamma}(n, \lambda)$, which makes sense in the Poisson process context.

## summary of the Bernoulli-Poisson-o-sphere

Starting with a Bernoulli($p$) process, we have the following:

\begin{table}[ht]
\begin{tabular}{|r|r|c|r|}\hline
What? & Discrete Version & Comments & Continuous Version\\\hline
Count & \onslide<2->{Binomial$(n,p)$} & 
\begin{minipage}{4cm}\raggedright \onslide<2->{Sum of $n$ Bernoulli($p$).}\\
               \onslide<6->{Fix $E(X(t)) = np = \lambda t$ fixed\\ $n\to\infty\ldots$}\end{minipage} & \onslide<6->{$\ldots\text{Poisson}(\lambda t)$} \\\hline
Inter-arrival & \onslide<3->{Geometric($p$)} & \onslide<3->{"Memoryless"} & \onslide<7->{Exponential($\lambda$)}\\ \hline
Wait for $r^{th}$ event & \onslide<4->{NegBin$(r,p)$} & & \onslide<8->{Gamma$(r,\lambda)$}\\\hline
Look back after 1 & \onslide<5->{"Discrete Uniform"} & \onslide<5->{($\leftarrow$ not done)} & \onslide<9->{Uniform(0,$t$)}\\\hline
\end{tabular}
\end{table}

## a re-parametrization

There was nothing sacred about using $\lambda$ in the definitions of the exponential and gamma distributions.

\pause Any 1-1 function of a parameter will do. For example, it is common to "parametrize" the exponential and gamma distributions by $\beta = 1/\lambda$ instead.

\pause $\beta$ is called the "mean" or "scale" parameter, in this case.

\pause This is the book's parametrization, despite being an engineering book. 

\pause But it doesn't matter.

\pause Although it makes me scarlet with rage.

# the "normal" distributions

## distributions of sums

We've already seen many examples of things that can be considered as \textit{sums of random variables}.

\pause Binomial

\pause Negative Binomial

\pause Gamma

\pause Even Poisson, in the broader sense of "summing up events over an interval"

\pause Let's see what happens with the sum is of not a small number of terms...

## Binomial(30, 0.5)

```{r}
n <- 30
p <- 0.5
data_frame(x = trunc(seq(n*p - 4*sqrt(n*p*(1-p)), 
                                  n*p + 4*sqrt(n*p*(1-p)), by=1))) %>% 
  mutate(pmf = dbinom(x,n,p)) %>% 
  ggplot(aes(x=x, y=pmf)) + geom_point()
```

## NegBin(40, 0.5)

```{r}
n <- 40
p <- 0.5
data_frame(x = trunc(seq(n/p - 4*sqrt(n*(1-p)/p^2), 
                                  n/p + 4*sqrt(n*(1-p)/p^2), by=1))) %>% 
  mutate(pmf = dnbinom(x-n, n, p)) %>% 
  ggplot(aes(x=x, y=pmf)) + geom_point()
```

## Gamma(35, 2)

```{r}
n <- 35
lambda <- 2
data_frame(x = seq(n/lambda - 4*sqrt(n/lambda^2), 
                                  n/lambda + 4*sqrt(n/lambda^2), by=0.1)) %>% 
  mutate(pdf = dgamma(x,n,lambda)) %>% 
  ggplot(aes(x=x, y=pdf)) + geom_line()
```

## Poisson(50)

```{r}
lambda <- 50
data_frame(x = trunc(seq(lambda - 4*sqrt(lambda), 
                                  lambda + 4*sqrt(lambda), by=1))) %>% 
  mutate(pmf = dpois(x, lambda)) %>% 
  ggplot(aes(x=x, y=pmf)) + geom_point()
```


## normal distributions "in the wild"

Some things actually have normal distributions (symmetric, bell-shaped, no extreme values) in and of themselves.

\pause Such as when the random "thing" is the combination of:

* not a small number of...

* ...roughly equally weighted

* ... mostly independent...

* ...other random things.


\pause Height, test score, lab measurement, etc.

\pause But this fact undersells the critical importance of the normal distributions.

## the normal distributions

We say $Z$ has a "standard" normal distribution, or $Z\sim N(0,1)$, if its density is:
$$f(z) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}z^2},\qquad -\infty < z < \infty$$

\pause Is this a density?

\pause $E(Z)=0$ (easy integral) and $\V{Z}=1$ (easy integration by parts).

\pause Since $\int\limits_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}z^2}\,dz = 1$, the change of variables $z = \frac{x-\mu}{\sigma}$ for any $\mu$ and any $\sigma > 0$ shows the following is also a valid density:
$$f(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}$$
\pause We say $X$ has a normal distribution with parameters $\mu$ and $\sigma$, or $X \sim N(\mu, \sigma)$, when it has this density. 

